{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c461369d-72fd-474e-991b-dac3d8946686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384388ec-db36-4f26-82c4-f6ce90ecdc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65912665-bfb9-4034-b0ca-606f09e8c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Downloading comtypes-1.4.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pyttsx3) (306)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Downloading comtypes-1.4.2-py3-none-any.whl (201 kB)\n",
      "   ---------------------------------------- 0.0/201.2 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 71.7/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 143.4/201.2 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 194.6/201.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 201.2/201.2 kB 408.1 kB/s eta 0:00:00\n",
      "Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: comtypes, pypiwin32, pyttsx3\n",
      "Successfully installed comtypes-1.4.2 pypiwin32-223 pyttsx3-2.90\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn pyttsx3 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc06d40-d144-48bf-88ec-6fe7c416d15c",
   "metadata": {},
   "source": [
    "# mainly includes 3 steps:\n",
    "\n",
    "* capturing hand landarks -We use MediaPipe to detect and track hand landmarks from the webcam feed.Each frame's hand landmarks are collected and saved into a CSV file for later use in model training.\n",
    "* training a machine learning model: we train a K-Nearest Neighbors (KNN) classifier to recognize different hand gestures.\n",
    "  We load the collected hand landmarks data.\n",
    "  Split the data into training and testing sets.\n",
    "  Train a KNN classifier and evaluate its accuracy.\n",
    "  Save the trained model for later use in real-time recognition.\n",
    "* recognising gestures in real time\n",
    "  We load the trained model.\n",
    "  Capture hand landmarks in real-time and use the model to predict the gesture.\n",
    "  Provide audio feedback using text-to-speech and display the gesture name on the webcam feed.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2927098-edc7-446b-ab87-b02404b8e4a5",
   "metadata": {},
   "source": [
    "#  step 1:Capture Hand Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64bed01-9f4f-4ac2-8e04-ffbf84a59a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles \n",
    "\n",
    "# Initialize webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,  \n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "    while cam.isOpened():\n",
    "        success, image = cam.read()\n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        results = hands.process(image) #rocesses the RGB image to detect and track hands.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)#Converts the image back to BGR color space for OpenCV compatibility.\n",
    "        \n",
    "       \n",
    "        if results.multi_hand_landmarks: #Checks if any hand landmarks are detected.\n",
    "            for hand_landmarks in results.multi_hand_landmarks: #Iterates through each detected hand.\n",
    "                mp_drawing.draw_landmarks( #Draws hand landmarks and connections on the image.\n",
    "                    image, \n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "             \n",
    "                data = [] #Initializes an empty list to store landmark coordinat\n",
    "                for point in mp_hands.HandLandmark:\n",
    "                    normalizedLandmark = hand_landmarks.landmark[point] #Gets the normalized coordinates of each landmark.\n",
    "                    data.append(normalizedLandmark.x) #Appends the x, y, and z coordinates of each landmark to the data list.\n",
    "                    data.append(normalizedLandmark.y)\n",
    "                    data.append(normalizedLandmark.z)\n",
    "            \n",
    "                print(len(data))\n",
    "                \n",
    "                \n",
    "                data = str(data)[1:-1] #Converts the data list to a comma-separated string and removes the square brackets.\n",
    "                with open('hello.csv', 'a') as f: #pens the CSV file in append mode\n",
    "                    f.write(data + ',hello\\n') #Writes the data string to the file, adding a label (\"rock\").\n",
    "        \n",
    "        cv2.imshow('Hand Tracking', image)\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0ac4c-e7ff-462e-bf5a-190cadab4e54",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383dec06-ae47-4f8f-8e02-4c6b49f36fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "data = pd.read_csv('hello.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25) #(25% for testing).\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5) # 5 neighbours\n",
    "classifier.fit(X_train, Y_train) #Trains the classifier on the training data.\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(Y_test, classifier.predict(X_test))\n",
    "print(f'Accuracy: {accuracy}') # accuracy of model\n",
    "\n",
    "# Save the model to a file\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(classifier, model_file) #Serializes and saves the trained classifier to the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b05fe0-13bf-4d4c-8368-ecec1af5af0e",
   "metadata": {},
   "source": [
    "#  load the trained model and run the application to recognize gestures and provide text-to-speech output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6e687b-fc69-4159-8a5c-5649da81f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "rock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pyttsx3  # the text-to-speech conversion library\n",
    "\n",
    "model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "engine = pyttsx3.init() # Initialize text-to-speech engine\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "    while cam.isOpened():\n",
    "        success, image = cam.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        \n",
    "     \n",
    "        imageWidth, imageHeight = image.shape[:2] #Retrieves height and width of img\n",
    "        \n",
    "        \n",
    "        image.flags.writeable = False #Marks the image as non-writable to improve performance by allowing operations to pass by reference instead of making a copy.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True # Marks the image as writable again after processing.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "                \n",
    "                \n",
    "                data = []\n",
    "                for point in mp_hands.HandLandmark:\n",
    "                    normalizedLandmark = hand_landmarks.landmark[point]\n",
    "                    data.append(normalizedLandmark.x)\n",
    "                    data.append(normalizedLandmark.y)\n",
    "                    data.append(normalizedLandmark.z)\n",
    "                \n",
    "                #Ensures that the data list contains the correct number of coordinates (21 landmarks × 3 coordinates each = 63\n",
    "                if len(data) == 63:\n",
    "                    \n",
    "                    out = model.predict([data])\n",
    "                    gesture = out[0]  # Assuming the model returns the gesture name\n",
    "                    \n",
    "                    print(gesture)\n",
    "                    \n",
    "                   \n",
    "                    engine.say(gesture) # text-to-speech engine to say the predicted gesture.\n",
    "                    engine.runAndWait() #Runs the speech engine to produce the sound.\n",
    "                    \n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    org = (50, 50) # top left corner of text\n",
    "                    fontScale = 1 # size of text\n",
    "                    color = (255, 0, 0) \n",
    "                    thickness = 2\n",
    "                    image = cv2.putText(image, gesture, org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "       \n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "       \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    " \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fbeda-2e00-48f1-afa8-21abaabad940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e36ab-63ee-48c7-8030-5a88adda30a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
